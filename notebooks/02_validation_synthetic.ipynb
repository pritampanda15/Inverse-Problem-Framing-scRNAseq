{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation with Synthetic Data\n",
    "\n",
    "Comprehensive validation of the inverse problem approach using synthetic data with known ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import inverse_sc as isc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate Data with Different Difficulty Levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenarios = ['simple', 'moderate', 'hard']\n",
    "datasets = {}\n",
    "\n",
    "for scenario in scenarios:\n",
    "    adata, truth = isc.validation.generate_realistic_benchmark(scenario=scenario)\n",
    "    datasets[scenario] = (adata, truth)\n",
    "    print(f\"{scenario}: {adata.shape}, dropout={(adata.X == 0).mean():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Fit Models on Each Scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_all = []\n",
    "\n",
    "for scenario, (adata, truth) in datasets.items():\n",
    "    print(f\"\\n=== {scenario.upper()} SCENARIO ===\")\n",
    "    \n",
    "    # Fit inverse model\n",
    "    isc.pp.fit_inverse_model(\n",
    "        adata,\n",
    "        n_epochs=150,\n",
    "        batch_size=256,\n",
    "    )\n",
    "    \n",
    "    # Benchmark\n",
    "    results = isc.validation.benchmark_against_scanpy(adata, truth, run_scanpy=True)\n",
    "    results['scenario'] = scenario\n",
    "    results_all.append(results)\n",
    "    \n",
    "    # Calibration\n",
    "    calib = isc.validation.uncertainty_calibration(adata, truth)\n",
    "    print(f\"Calibration score: {calib['calibration_score']:.3f}\")\n",
    "\n",
    "results_df = pd.concat(results_all, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot correlations across scenarios\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for ax, metric in zip(axes, ['mean_cell_correlation', 'global_correlation', 'rmse']):\n",
    "    pivot = results_df.pivot(index='scenario', columns='method', values=metric)\n",
    "    pivot.plot(kind='bar', ax=ax)\n",
    "    ax.set_title(metric)\n",
    "    ax.set_xlabel('Scenario')\n",
    "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Detailed Analysis: Moderate Scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata, truth = datasets['moderate']\n",
    "\n",
    "# Cell-wise correlation distribution\n",
    "Z_true = truth['Z_true']\n",
    "Z_inferred = adata.obsm['Z_true_mean']\n",
    "\n",
    "cell_corrs = []\n",
    "for i in range(Z_true.shape[0]):\n",
    "    corr = np.corrcoef(Z_true[i], Z_inferred[i])[0, 1]\n",
    "    cell_corrs.append(corr)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(cell_corrs, bins=50, edgecolor='black')\n",
    "plt.xlabel('Correlation (True vs Inferred)')\n",
    "plt.ylabel('Number of Cells')\n",
    "plt.title('Cell-wise Recovery Quality')\n",
    "plt.axvline(np.mean(cell_corrs), color='red', linestyle='--', label=f'Mean: {np.mean(cell_corrs):.3f}')\n",
    "plt.legend()\n",
    "\n",
    "# Scatter plot: true vs inferred (sample)\n",
    "plt.subplot(1, 2, 2)\n",
    "sample_cells = np.random.choice(Z_true.shape[0], 5, replace=False)\n",
    "for cell_idx in sample_cells:\n",
    "    plt.scatter(Z_true[cell_idx], Z_inferred[cell_idx], alpha=0.3, s=1)\n",
    "plt.plot([0, Z_true.max()], [0, Z_true.max()], 'r--', label='Perfect recovery')\n",
    "plt.xlabel('True Expression')\n",
    "plt.ylabel('Inferred Expression')\n",
    "plt.title('True vs Inferred (5 sample cells)')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Uncertainty Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_std = adata.obsm['Z_true_std']\n",
    "\n",
    "# Plot: where uncertainty is high, is recovery worse?\n",
    "avg_uncertainty = Z_std.mean(axis=1)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(avg_uncertainty, cell_corrs, alpha=0.5, s=10)\n",
    "plt.xlabel('Average Uncertainty')\n",
    "plt.ylabel('Recovery Correlation')\n",
    "plt.title('Uncertainty vs Recovery Quality')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "# Calibration check\n",
    "errors = np.abs(Z_true - Z_inferred)\n",
    "standardized_errors = errors / (Z_std + 1e-8)\n",
    "\n",
    "plt.hist(standardized_errors.flatten(), bins=50, edgecolor='black', alpha=0.7, density=True)\n",
    "x = np.linspace(0, 5, 100)\n",
    "plt.plot(x, np.exp(-x**2/2) / np.sqrt(2*np.pi), 'r-', linewidth=2, label='Standard Normal')\n",
    "plt.xlabel('Standardized Error')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Uncertainty Calibration')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Program Recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare inferred programs to true programs\n",
    "true_program_weights = truth['program_weights']\n",
    "inferred_program_weights = adata.obsm['program_weights']\n",
    "\n",
    "# Correlation matrix between true and inferred programs\n",
    "n_programs = true_program_weights.shape[1]\n",
    "corr_matrix = np.zeros((n_programs, n_programs))\n",
    "\n",
    "for i in range(n_programs):\n",
    "    for j in range(n_programs):\n",
    "        corr_matrix[i, j] = np.corrcoef(\n",
    "            true_program_weights[:, i],\n",
    "            inferred_program_weights[:, j]\n",
    "        )[0, 1]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='RdBu_r', center=0,\n",
    "            xticklabels=[f'Inf_{i}' for i in range(n_programs)],\n",
    "            yticklabels=[f'True_{i}' for i in range(n_programs)])\n",
    "plt.title('Program Recovery: True vs Inferred')\n",
    "plt.xlabel('Inferred Programs')\n",
    "plt.ylabel('True Programs')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nNote: High off-diagonal values may indicate label switching,\")\n",
    "print(\"which is expected (programs are unordered).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== VALIDATION SUMMARY ===\")\n",
    "print(\"\\nRecovery Quality:\")\n",
    "print(f\"  Mean cell correlation: {np.mean(cell_corrs):.3f} ± {np.std(cell_corrs):.3f}\")\n",
    "print(f\"  Global correlation: {np.corrcoef(Z_true.flatten(), Z_inferred.flatten())[0,1]:.3f}\")\n",
    "print(f\"  RMSE: {np.sqrt(np.mean((Z_true - Z_inferred)**2)):.3f}\")\n",
    "\n",
    "calib = isc.validation.uncertainty_calibration(adata, truth)\n",
    "print(\"\\nUncertainty Calibration:\")\n",
    "print(f\"  Coverage at 1σ: {calib['coverage_1std']:.3f} (target: 0.68)\")\n",
    "print(f\"  Coverage at 2σ: {calib['coverage_2std']:.3f} (target: 0.95)\")\n",
    "print(f\"  Calibration score: {calib['calibration_score']:.3f}\")\n",
    "\n",
    "print(\"\\n✓ Validation complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
